
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "auto_examples/unfolded/demo_custom_prior_unfolded.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_auto_examples_unfolded_demo_custom_prior_unfolded.py>`
        to download the full example code

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_auto_examples_unfolded_demo_custom_prior_unfolded.py:


Learned iterative custom prior
==============================

This example shows how to implement a learned unrolled proximal gradient descent algorithm with a custom prior function.
The algorithm is trained on a dataset of compressed sensing measurements of MNIST images.

.. GENERATED FROM PYTHON SOURCE LINES 9-20

.. code-block:: Python

    from pathlib import Path
    import torch
    from torchvision import datasets
    from torchvision import transforms
    import deepinv as dinv
    from torch.utils.data import DataLoader
    from deepinv.optim.data_fidelity import L2
    from deepinv.optim.prior import Prior
    from deepinv.unfolded import unfolded_builder
    from deepinv.training_utils import train, test








.. GENERATED FROM PYTHON SOURCE LINES 21-24

Setup paths for data loading and results.
-----------------------------------------


.. GENERATED FROM PYTHON SOURCE LINES 24-36

.. code-block:: Python


    BASE_DIR = Path(".")
    ORIGINAL_DATA_DIR = BASE_DIR / "datasets"
    DATA_DIR = BASE_DIR / "measurements"
    RESULTS_DIR = BASE_DIR / "results"
    CKPT_DIR = BASE_DIR / "ckpts"

    # Set the global random seed from pytorch to ensure reproducibility of the example.
    torch.manual_seed(0)

    device = dinv.utils.get_freer_gpu() if torch.cuda.is_available() else "cpu"








.. GENERATED FROM PYTHON SOURCE LINES 37-40

Load base image datasets and degradation operators.
---------------------------------------------------
In this example, we use MNIST as the base dataset.

.. GENERATED FROM PYTHON SOURCE LINES 40-56

.. code-block:: Python


    img_size = 28
    n_channels = 1
    operation = "compressed-sensing"
    train_dataset_name = "MNIST_train"

    # Generate training and evaluation datasets in HDF5 folders and load them.
    train_test_transform = transforms.Compose([transforms.ToTensor()])
    train_base_dataset = datasets.MNIST(
        root=ORIGINAL_DATA_DIR, train=True, transform=train_test_transform, download=True
    )
    test_base_dataset = datasets.MNIST(
        root=ORIGINAL_DATA_DIR, train=False, transform=train_test_transform, download=True
    )









.. GENERATED FROM PYTHON SOURCE LINES 57-64

Generate a dataset of compressed measurements and load it.
----------------------------------------------------------
We use the compressed sensing class from the physics module to generate a dataset of highly-compressed measurements
(10% of the total number of pixels).

The forward operator is defined as :math:`y = Ax`
where :math:`A` is a (normalized) random Gaussian matrix.

.. GENERATED FROM PYTHON SOURCE LINES 64-94

.. code-block:: Python



    # Use parallel dataloader if using a GPU to fasten training, otherwise, as all computes are on CPU, use synchronous
    # data loading.
    num_workers = 4 if torch.cuda.is_available() else 0

    # Generate the compressed sensing measurement operator with 10x under-sampling factor.
    physics = dinv.physics.CompressedSensing(
        m=78, img_shape=(n_channels, img_size, img_size), fast=True, device=device
    )
    my_dataset_name = "demo_LICP"
    n_images_max = (
        1000 if torch.cuda.is_available() else 200
    )  # maximal number of images used for training
    measurement_dir = DATA_DIR / train_dataset_name / operation
    generated_datasets_path = dinv.datasets.generate_dataset(
        train_dataset=train_base_dataset,
        test_dataset=test_base_dataset,
        physics=physics,
        device=device,
        save_dir=measurement_dir,
        train_datapoints=n_images_max,
        test_datapoints=8,
        num_workers=num_workers,
        dataset_filename=str(my_dataset_name),
    )

    train_dataset = dinv.datasets.HDF5Dataset(path=generated_datasets_path, train=True)
    test_dataset = dinv.datasets.HDF5Dataset(path=generated_datasets_path, train=False)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Computing train measurement vectors from base dataset...
      0%|          | 0/1 [00:00<?, ?it/s]    100%|██████████| 1/1 [00:00<00:00, 16.22it/s]
    Computing test measurement vectors from base dataset...
      0%|          | 0/2 [00:00<?, ?it/s]    100%|██████████| 2/2 [00:00<00:00, 728.05it/s]
    Dataset has been saved in measurements/MNIST_train/compressed-sensing




.. GENERATED FROM PYTHON SOURCE LINES 95-115

Define the unfolded Proximal Gradient algorithm.
------------------------------------------------
In this example, we propose to minimise a function of the form

.. math::

         \min_x \frac{\lambda}{2} \|y - Ax\|_2^2 + \operatorname{TV}_{\text{smooth}}(x)

where :math:`\operatorname{TV}_{\text{smooth}}` is a smooth approximation of TV.
The proximal gradient iteration (see also :class:`deepinv.optim.optim_iterators.PGDIteration`) is defined as

  .. math::
          x_{k+1} = \text{prox}_{\gamma \operatorname{TV}_{\text{smooth}}}(x_k - \gamma \lambda A^T (Ax_k - y))

where :math:`\gamma` is the stepsize and :math:`\text{prox}_{g}` is the proximity operator of :math:`g(x) =\operatorname{TV}_{\text{smooth}}(x)`.

We first define the prior in a functional form.
If the prior is initialized with a list of length max_iter,
then a distinct weight is trained for each PGD iteration.
For fixed trained model prior across iterations, initialize with a single model.

.. GENERATED FROM PYTHON SOURCE LINES 115-182

.. code-block:: Python



    # Define the image gradient operator
    def nabla(I):
        b, c, h, w = I.shape
        G = torch.zeros((b, c, h, w, 2), device=I.device).type(I.dtype)
        G[:, :, :-1, :, 0] = G[:, :, :-1, :, 0] - I[:, :, :-1]
        G[:, :, :-1, :, 0] = G[:, :, :-1, :, 0] + I[:, :, 1:]
        G[:, :, :, :-1, 1] = G[:, :, :, :-1, 1] - I[..., :-1]
        G[:, :, :, :-1, 1] = G[:, :, :, :-1, 1] + I[..., 1:]
        return G


    # Define the smooth TV prior as the mse of the image finite difference.
    def g(x, *args):
        dx = nabla(x)
        tv_smooth = torch.nn.functional.mse_loss(
            dx, torch.zeros_like(dx), reduction="sum"
        ).sqrt()
        return tv_smooth


    # Define the prior. A prior instance from :class:`deepinv.priors` can be simply defined with an explicit potential :math:`g` function as such:
    prior = Prior(g=g)

    # We use :meth:`deepinv.unfolded.unfolded_builder` to define the unfolded algorithm
    # and set both the stepsizes of the PGD algorithm :math:`\gamma` (``stepsize``) and the soft
    # thresholding parameters :math:`\lambda` as learnable parameters.
    # These parameters are initialized with a table of length max_iter,
    # yielding a distinct ``stepsize`` and ``g_param`` value for each iteration of the algorithm.
    # For single ``stepsize`` and ``g_param`` shared across iterations, initialize with a single float value.

    # Unrolled optimization algorithm parameters
    max_iter = 5
    lamb = [
        1.0
    ] * max_iter  # initialization of the regularization parameter. A distinct lamb is trained for each iteration.
    stepsize = [
        1.0
    ] * max_iter  # initialization of the stepsizes. A distinct stepsize is trained for each iteration.
    params_algo = {  # wrap all the restoration parameters in a 'params_algo' dictionary
        "stepsize": stepsize,
        "lambda": lamb,
    }
    trainable_params = [
        "stepsize",
        "lambda",
    ]  # define which parameters from 'params_algo' are trainable

    # Select the data fidelity term
    data_fidelity = L2()

    # Logging parameters
    verbose = True
    wandb_vis = False  # plot curves and images in Weight&Bias

    # Define the unfolded trainable model.
    model = unfolded_builder(
        iteration="PGD",
        params_algo=params_algo.copy(),
        trainable_params=trainable_params,
        data_fidelity=data_fidelity,
        max_iter=max_iter,
        prior=prior,
        g_first=False,
    )








.. GENERATED FROM PYTHON SOURCE LINES 183-187

Define the training parameters.
-------------------------------
We now define training-related parameters,
number of epochs, optimizer (Adam) and its hyperparameters, and the train and test batch sizes.

.. GENERATED FROM PYTHON SOURCE LINES 187-210

.. code-block:: Python



    # Training parameters
    epochs = 10 if torch.cuda.is_available() else 5
    learning_rate = 1e-2

    # Choose optimizer and scheduler
    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=0.0)

    # Choose supervised training loss
    losses = [dinv.loss.SupLoss(metric=dinv.metric.mse())]

    # Batch sizes and data loaders
    train_batch_size = 64 if torch.cuda.is_available() else 8
    test_batch_size = 64 if torch.cuda.is_available() else 8

    train_dataloader = DataLoader(
        train_dataset, batch_size=train_batch_size, num_workers=num_workers, shuffle=True
    )
    test_dataloader = DataLoader(
        test_dataset, batch_size=test_batch_size, num_workers=num_workers, shuffle=False
    )








.. GENERATED FROM PYTHON SOURCE LINES 211-215

Train the network.
------------------
We train the network using the library's train function.


.. GENERATED FROM PYTHON SOURCE LINES 215-230

.. code-block:: Python


    train(
        model=model,
        train_dataloader=train_dataloader,
        eval_dataloader=test_dataloader,
        epochs=epochs,
        losses=losses,
        physics=physics,
        optimizer=optimizer,
        device=device,
        save_path=str(CKPT_DIR / operation),
        verbose=verbose,
        wandb_vis=wandb_vis,  # training visualization can be done in Weight&Bias
    )





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    The model has 10 trainable parameters
      0%|          | 0/25 [00:00<?, ?it/s]    Epoch 1:   0%|          | 0/25 [00:00<?, ?it/s]    Epoch 1:   0%|          | 0/25 [00:00<?, ?it/s, eval_psnr=12, total_loss=0.0762, train_psnr=11.5]    Epoch 1:   4%|▍         | 1/25 [00:00<00:09,  2.56it/s, eval_psnr=12, total_loss=0.0762, train_psnr=11.5]    Epoch 1:   4%|▍         | 1/25 [00:00<00:09,  2.56it/s, eval_psnr=12, total_loss=0.0762, train_psnr=11.5]    Epoch 1:   4%|▍         | 1/25 [00:00<00:09,  2.56it/s, eval_psnr=12, total_loss=0.0872, train_psnr=10.9]    Epoch 1:   8%|▊         | 2/25 [00:00<00:05,  4.28it/s, eval_psnr=12, total_loss=0.0872, train_psnr=10.9]    Epoch 1:   8%|▊         | 2/25 [00:00<00:05,  4.28it/s, eval_psnr=12, total_loss=0.0872, train_psnr=10.9]    Epoch 1:   8%|▊         | 2/25 [00:00<00:05,  4.28it/s, eval_psnr=12, total_loss=0.0813, train_psnr=11.2]    Epoch 1:  12%|█▏        | 3/25 [00:00<00:07,  2.93it/s, eval_psnr=12, total_loss=0.0813, train_psnr=11.2]    Epoch 1:  12%|█▏        | 3/25 [00:00<00:07,  2.93it/s, eval_psnr=12, total_loss=0.0813, train_psnr=11.2]    Epoch 1:  12%|█▏        | 3/25 [00:01<00:07,  2.93it/s, eval_psnr=12, total_loss=0.0847, train_psnr=11.1]    Epoch 1:  16%|█▌        | 4/25 [00:01<00:06,  3.39it/s, eval_psnr=12, total_loss=0.0847, train_psnr=11.1]    Epoch 1:  16%|█▌        | 4/25 [00:01<00:06,  3.39it/s, eval_psnr=12, total_loss=0.0847, train_psnr=11.1]    Epoch 1:  16%|█▌        | 4/25 [00:01<00:06,  3.39it/s, eval_psnr=12, total_loss=0.0829, train_psnr=11.1]    Epoch 1:  20%|██        | 5/25 [00:01<00:06,  2.88it/s, eval_psnr=12, total_loss=0.0829, train_psnr=11.1]    Epoch 1:  20%|██        | 5/25 [00:01<00:06,  2.88it/s, eval_psnr=12, total_loss=0.0829, train_psnr=11.1]    Epoch 1:  20%|██        | 5/25 [00:01<00:06,  2.88it/s, eval_psnr=12, total_loss=0.0855, train_psnr=11]      Epoch 1:  24%|██▍       | 6/25 [00:01<00:06,  3.16it/s, eval_psnr=12, total_loss=0.0855, train_psnr=11]    Epoch 1:  24%|██▍       | 6/25 [00:01<00:06,  3.16it/s, eval_psnr=12, total_loss=0.0855, train_psnr=11]    Epoch 1:  24%|██▍       | 6/25 [00:02<00:06,  3.16it/s, eval_psnr=12, total_loss=0.0826, train_psnr=11.1]    Epoch 1:  28%|██▊       | 7/25 [00:02<00:06,  2.74it/s, eval_psnr=12, total_loss=0.0826, train_psnr=11.1]    Epoch 1:  28%|██▊       | 7/25 [00:02<00:06,  2.74it/s, eval_psnr=12, total_loss=0.0826, train_psnr=11.1]    Epoch 1:  28%|██▊       | 7/25 [00:02<00:06,  2.74it/s, eval_psnr=12, total_loss=0.081, train_psnr=11.2]     Epoch 1:  32%|███▏      | 8/25 [00:02<00:06,  2.49it/s, eval_psnr=12, total_loss=0.081, train_psnr=11.2]    Epoch 1:  32%|███▏      | 8/25 [00:02<00:06,  2.49it/s, eval_psnr=12, total_loss=0.081, train_psnr=11.2]    Epoch 1:  32%|███▏      | 8/25 [00:03<00:06,  2.49it/s, eval_psnr=12, total_loss=0.0807, train_psnr=11.2]    Epoch 1:  36%|███▌      | 9/25 [00:03<00:05,  2.68it/s, eval_psnr=12, total_loss=0.0807, train_psnr=11.2]    Epoch 1:  36%|███▌      | 9/25 [00:03<00:05,  2.68it/s, eval_psnr=12, total_loss=0.0807, train_psnr=11.2]    Epoch 1:  36%|███▌      | 9/25 [00:03<00:05,  2.68it/s, eval_psnr=12, total_loss=0.0814, train_psnr=11.2]    Epoch 1:  40%|████      | 10/25 [00:03<00:05,  2.62it/s, eval_psnr=12, total_loss=0.0814, train_psnr=11.2]    Epoch 1:  40%|████      | 10/25 [00:03<00:05,  2.62it/s, eval_psnr=12, total_loss=0.0814, train_psnr=11.2]    Epoch 1:  40%|████      | 10/25 [00:03<00:05,  2.62it/s, eval_psnr=12, total_loss=0.0824, train_psnr=11.1]    Epoch 1:  44%|████▍     | 11/25 [00:03<00:04,  2.91it/s, eval_psnr=12, total_loss=0.0824, train_psnr=11.1]    Epoch 1:  44%|████▍     | 11/25 [00:03<00:04,  2.91it/s, eval_psnr=12, total_loss=0.0824, train_psnr=11.1]    Epoch 1:  44%|████▍     | 11/25 [00:04<00:04,  2.91it/s, eval_psnr=12, total_loss=0.082, train_psnr=11.1]     Epoch 1:  48%|████▊     | 12/25 [00:04<00:04,  2.61it/s, eval_psnr=12, total_loss=0.082, train_psnr=11.1]    Epoch 1:  48%|████▊     | 12/25 [00:04<00:04,  2.61it/s, eval_psnr=12, total_loss=0.082, train_psnr=11.1]    Epoch 1:  48%|████▊     | 12/25 [00:04<00:04,  2.61it/s, eval_psnr=12, total_loss=0.0833, train_psnr=11.1]    Epoch 1:  52%|█████▏    | 13/25 [00:04<00:04,  2.90it/s, eval_psnr=12, total_loss=0.0833, train_psnr=11.1]    Epoch 1:  52%|█████▏    | 13/25 [00:04<00:04,  2.90it/s, eval_psnr=12, total_loss=0.0833, train_psnr=11.1]    Epoch 1:  52%|█████▏    | 13/25 [00:04<00:04,  2.90it/s, eval_psnr=12, total_loss=0.0837, train_psnr=11.1]    Epoch 1:  56%|█████▌    | 14/25 [00:04<00:03,  2.91it/s, eval_psnr=12, total_loss=0.0837, train_psnr=11.1]    Epoch 1:  56%|█████▌    | 14/25 [00:04<00:03,  2.91it/s, eval_psnr=12, total_loss=0.0837, train_psnr=11.1]    Epoch 1:  56%|█████▌    | 14/25 [00:05<00:03,  2.91it/s, eval_psnr=12, total_loss=0.0833, train_psnr=11.1]    Epoch 1:  60%|██████    | 15/25 [00:05<00:03,  2.63it/s, eval_psnr=12, total_loss=0.0833, train_psnr=11.1]    Epoch 1:  60%|██████    | 15/25 [00:05<00:03,  2.63it/s, eval_psnr=12, total_loss=0.0833, train_psnr=11.1]    Epoch 1:  60%|██████    | 15/25 [00:05<00:03,  2.63it/s, eval_psnr=12, total_loss=0.0836, train_psnr=11.1]    Epoch 1:  64%|██████▍   | 16/25 [00:05<00:03,  2.44it/s, eval_psnr=12, total_loss=0.0836, train_psnr=11.1]    Epoch 1:  64%|██████▍   | 16/25 [00:05<00:03,  2.44it/s, eval_psnr=12, total_loss=0.0836, train_psnr=11.1]    Epoch 1:  64%|██████▍   | 16/25 [00:06<00:03,  2.44it/s, eval_psnr=12, total_loss=0.0839, train_psnr=11.1]    Epoch 1:  68%|██████▊   | 17/25 [00:06<00:03,  2.52it/s, eval_psnr=12, total_loss=0.0839, train_psnr=11.1]    Epoch 1:  68%|██████▊   | 17/25 [00:06<00:03,  2.52it/s, eval_psnr=12, total_loss=0.0839, train_psnr=11.1]    Epoch 1:  68%|██████▊   | 17/25 [00:06<00:03,  2.52it/s, eval_psnr=12, total_loss=0.0839, train_psnr=11]      Epoch 1:  72%|███████▏  | 18/25 [00:06<00:02,  2.48it/s, eval_psnr=12, total_loss=0.0839, train_psnr=11]    Epoch 1:  72%|███████▏  | 18/25 [00:06<00:02,  2.48it/s, eval_psnr=12, total_loss=0.0839, train_psnr=11]    Epoch 1:  72%|███████▏  | 18/25 [00:07<00:02,  2.48it/s, eval_psnr=12, total_loss=0.0848, train_psnr=11]    Epoch 1:  76%|███████▌  | 19/25 [00:07<00:02,  2.47it/s, eval_psnr=12, total_loss=0.0848, train_psnr=11]    Epoch 1:  76%|███████▌  | 19/25 [00:07<00:02,  2.47it/s, eval_psnr=12, total_loss=0.0848, train_psnr=11]    Epoch 1:  76%|███████▌  | 19/25 [00:07<00:02,  2.47it/s, eval_psnr=12, total_loss=0.0855, train_psnr=10.9]    Epoch 1:  80%|████████  | 20/25 [00:07<00:01,  2.81it/s, eval_psnr=12, total_loss=0.0855, train_psnr=10.9]    Epoch 1:  80%|████████  | 20/25 [00:07<00:01,  2.81it/s, eval_psnr=12, total_loss=0.0855, train_psnr=10.9]    Epoch 1:  80%|████████  | 20/25 [00:07<00:01,  2.81it/s, eval_psnr=12, total_loss=0.0854, train_psnr=10.9]    Epoch 1:  84%|████████▍ | 21/25 [00:07<00:01,  2.62it/s, eval_psnr=12, total_loss=0.0854, train_psnr=10.9]    Epoch 1:  84%|████████▍ | 21/25 [00:07<00:01,  2.62it/s, eval_psnr=12, total_loss=0.0854, train_psnr=10.9]    Epoch 1:  84%|████████▍ | 21/25 [00:08<00:01,  2.62it/s, eval_psnr=12, total_loss=0.085, train_psnr=11]       Epoch 1:  88%|████████▊ | 22/25 [00:08<00:01,  2.45it/s, eval_psnr=12, total_loss=0.085, train_psnr=11]    Epoch 1:  88%|████████▊ | 22/25 [00:08<00:01,  2.45it/s, eval_psnr=12, total_loss=0.085, train_psnr=11]    Epoch 1:  88%|████████▊ | 22/25 [00:08<00:01,  2.45it/s, eval_psnr=12, total_loss=0.0852, train_psnr=10.9]    Epoch 1:  92%|█████████▏| 23/25 [00:08<00:00,  2.56it/s, eval_psnr=12, total_loss=0.0852, train_psnr=10.9]    Epoch 1:  92%|█████████▏| 23/25 [00:08<00:00,  2.56it/s, eval_psnr=12, total_loss=0.0852, train_psnr=10.9]    Epoch 1:  92%|█████████▏| 23/25 [00:09<00:00,  2.56it/s, eval_psnr=12, total_loss=0.0854, train_psnr=10.9]    Epoch 1:  96%|█████████▌| 24/25 [00:09<00:00,  2.40it/s, eval_psnr=12, total_loss=0.0854, train_psnr=10.9]    Epoch 1:  96%|█████████▌| 24/25 [00:09<00:00,  2.40it/s, eval_psnr=12, total_loss=0.0854, train_psnr=10.9]    Epoch 1:  96%|█████████▌| 24/25 [00:09<00:00,  2.40it/s, eval_psnr=12, total_loss=0.0848, train_psnr=11]      Epoch 1: 100%|██████████| 25/25 [00:09<00:00,  2.28it/s, eval_psnr=12, total_loss=0.0848, train_psnr=11]    Epoch 1: 100%|██████████| 25/25 [00:09<00:00,  2.63it/s, eval_psnr=12, total_loss=0.0848, train_psnr=11]
      0%|          | 0/25 [00:00<?, ?it/s]    Epoch 2:   0%|          | 0/25 [00:00<?, ?it/s]    Epoch 2:   0%|          | 0/25 [00:00<?, ?it/s, eval_psnr=12.1, total_loss=0.0696, train_psnr=12.1]    Epoch 2:   4%|▍         | 1/25 [00:00<00:11,  2.07it/s, eval_psnr=12.1, total_loss=0.0696, train_psnr=12.1]    Epoch 2:   4%|▍         | 1/25 [00:00<00:11,  2.07it/s, eval_psnr=12.1, total_loss=0.0696, train_psnr=12.1]    Epoch 2:   4%|▍         | 1/25 [00:00<00:11,  2.07it/s, eval_psnr=12.1, total_loss=0.0802, train_psnr=11.3]    Epoch 2:   8%|▊         | 2/25 [00:00<00:11,  2.05it/s, eval_psnr=12.1, total_loss=0.0802, train_psnr=11.3]    Epoch 2:   8%|▊         | 2/25 [00:00<00:11,  2.05it/s, eval_psnr=12.1, total_loss=0.0802, train_psnr=11.3]    Epoch 2:   8%|▊         | 2/25 [00:01<00:11,  2.05it/s, eval_psnr=12.1, total_loss=0.0854, train_psnr=11]      Epoch 2:  12%|█▏        | 3/25 [00:01<00:10,  2.05it/s, eval_psnr=12.1, total_loss=0.0854, train_psnr=11]    Epoch 2:  12%|█▏        | 3/25 [00:01<00:10,  2.05it/s, eval_psnr=12.1, total_loss=0.0854, train_psnr=11]    Epoch 2:  12%|█▏        | 3/25 [00:01<00:10,  2.05it/s, eval_psnr=12.1, total_loss=0.0856, train_psnr=11]    Epoch 2:  16%|█▌        | 4/25 [00:01<00:10,  2.06it/s, eval_psnr=12.1, total_loss=0.0856, train_psnr=11]    Epoch 2:  16%|█▌        | 4/25 [00:01<00:10,  2.06it/s, eval_psnr=12.1, total_loss=0.0856, train_psnr=11]    Epoch 2:  16%|█▌        | 4/25 [00:02<00:10,  2.06it/s, eval_psnr=12.1, total_loss=0.0844, train_psnr=11]    Epoch 2:  20%|██        | 5/25 [00:02<00:09,  2.07it/s, eval_psnr=12.1, total_loss=0.0844, train_psnr=11]    Epoch 2:  20%|██        | 5/25 [00:02<00:09,  2.07it/s, eval_psnr=12.1, total_loss=0.0844, train_psnr=11]    Epoch 2:  20%|██        | 5/25 [00:02<00:09,  2.07it/s, eval_psnr=12.1, total_loss=0.0868, train_psnr=10.9]    Epoch 2:  24%|██▍       | 6/25 [00:02<00:09,  2.06it/s, eval_psnr=12.1, total_loss=0.0868, train_psnr=10.9]    Epoch 2:  24%|██▍       | 6/25 [00:02<00:09,  2.06it/s, eval_psnr=12.1, total_loss=0.0868, train_psnr=10.9]    Epoch 2:  24%|██▍       | 6/25 [00:03<00:09,  2.06it/s, eval_psnr=12.1, total_loss=0.0875, train_psnr=10.9]    Epoch 2:  28%|██▊       | 7/25 [00:03<00:08,  2.06it/s, eval_psnr=12.1, total_loss=0.0875, train_psnr=10.9]    Epoch 2:  28%|██▊       | 7/25 [00:03<00:08,  2.06it/s, eval_psnr=12.1, total_loss=0.0875, train_psnr=10.9]    Epoch 2:  28%|██▊       | 7/25 [00:03<00:08,  2.06it/s, eval_psnr=12.1, total_loss=0.0882, train_psnr=10.9]    Epoch 2:  32%|███▏      | 8/25 [00:03<00:08,  2.06it/s, eval_psnr=12.1, total_loss=0.0882, train_psnr=10.9]    Epoch 2:  32%|███▏      | 8/25 [00:03<00:08,  2.06it/s, eval_psnr=12.1, total_loss=0.0882, train_psnr=10.9]    Epoch 2:  32%|███▏      | 8/25 [00:04<00:08,  2.06it/s, eval_psnr=12.1, total_loss=0.0886, train_psnr=10.8]    Epoch 2:  36%|███▌      | 9/25 [00:04<00:07,  2.07it/s, eval_psnr=12.1, total_loss=0.0886, train_psnr=10.8]    Epoch 2:  36%|███▌      | 9/25 [00:04<00:07,  2.07it/s, eval_psnr=12.1, total_loss=0.0886, train_psnr=10.8]    Epoch 2:  36%|███▌      | 9/25 [00:04<00:07,  2.07it/s, eval_psnr=12.1, total_loss=0.0896, train_psnr=10.8]    Epoch 2:  40%|████      | 10/25 [00:04<00:07,  2.07it/s, eval_psnr=12.1, total_loss=0.0896, train_psnr=10.8]    Epoch 2:  40%|████      | 10/25 [00:04<00:07,  2.07it/s, eval_psnr=12.1, total_loss=0.0896, train_psnr=10.8]    Epoch 2:  40%|████      | 10/25 [00:05<00:07,  2.07it/s, eval_psnr=12.1, total_loss=0.0889, train_psnr=10.8]    Epoch 2:  44%|████▍     | 11/25 [00:05<00:06,  2.07it/s, eval_psnr=12.1, total_loss=0.0889, train_psnr=10.8]    Epoch 2:  44%|████▍     | 11/25 [00:05<00:06,  2.07it/s, eval_psnr=12.1, total_loss=0.0889, train_psnr=10.8]    Epoch 2:  44%|████▍     | 11/25 [00:05<00:06,  2.07it/s, eval_psnr=12.1, total_loss=0.0878, train_psnr=10.9]    Epoch 2:  48%|████▊     | 12/25 [00:05<00:06,  2.08it/s, eval_psnr=12.1, total_loss=0.0878, train_psnr=10.9]    Epoch 2:  48%|████▊     | 12/25 [00:05<00:06,  2.08it/s, eval_psnr=12.1, total_loss=0.0878, train_psnr=10.9]    Epoch 2:  48%|████▊     | 12/25 [00:06<00:06,  2.08it/s, eval_psnr=12.1, total_loss=0.0873, train_psnr=10.9]    Epoch 2:  52%|█████▏    | 13/25 [00:06<00:05,  2.07it/s, eval_psnr=12.1, total_loss=0.0873, train_psnr=10.9]    Epoch 2:  52%|█████▏    | 13/25 [00:06<00:05,  2.07it/s, eval_psnr=12.1, total_loss=0.0873, train_psnr=10.9]    Epoch 2:  52%|█████▏    | 13/25 [00:06<00:05,  2.07it/s, eval_psnr=12.1, total_loss=0.0856, train_psnr=11]      Epoch 2:  56%|█████▌    | 14/25 [00:06<00:05,  2.06it/s, eval_psnr=12.1, total_loss=0.0856, train_psnr=11]    Epoch 2:  56%|█████▌    | 14/25 [00:06<00:05,  2.06it/s, eval_psnr=12.1, total_loss=0.0856, train_psnr=11]    Epoch 2:  56%|█████▌    | 14/25 [00:07<00:05,  2.06it/s, eval_psnr=12.1, total_loss=0.0859, train_psnr=11]    Epoch 2:  60%|██████    | 15/25 [00:07<00:04,  2.04it/s, eval_psnr=12.1, total_loss=0.0859, train_psnr=11]    Epoch 2:  60%|██████    | 15/25 [00:07<00:04,  2.04it/s, eval_psnr=12.1, total_loss=0.0859, train_psnr=11]    Epoch 2:  60%|██████    | 15/25 [00:07<00:04,  2.04it/s, eval_psnr=12.1, total_loss=0.0856, train_psnr=11]    Epoch 2:  64%|██████▍   | 16/25 [00:07<00:04,  2.04it/s, eval_psnr=12.1, total_loss=0.0856, train_psnr=11]    Epoch 2:  64%|██████▍   | 16/25 [00:07<00:04,  2.04it/s, eval_psnr=12.1, total_loss=0.0856, train_psnr=11]    Epoch 2:  64%|██████▍   | 16/25 [00:08<00:04,  2.04it/s, eval_psnr=12.1, total_loss=0.0845, train_psnr=11]    Epoch 2:  68%|██████▊   | 17/25 [00:08<00:03,  2.04it/s, eval_psnr=12.1, total_loss=0.0845, train_psnr=11]    Epoch 2:  68%|██████▊   | 17/25 [00:08<00:03,  2.04it/s, eval_psnr=12.1, total_loss=0.0845, train_psnr=11]    Epoch 2:  68%|██████▊   | 17/25 [00:08<00:03,  2.04it/s, eval_psnr=12.1, total_loss=0.0844, train_psnr=11]    Epoch 2:  72%|███████▏  | 18/25 [00:08<00:03,  2.04it/s, eval_psnr=12.1, total_loss=0.0844, train_psnr=11]    Epoch 2:  72%|███████▏  | 18/25 [00:08<00:03,  2.04it/s, eval_psnr=12.1, total_loss=0.0844, train_psnr=11]    Epoch 2:  72%|███████▏  | 18/25 [00:09<00:03,  2.04it/s, eval_psnr=12.1, total_loss=0.084, train_psnr=11]     Epoch 2:  76%|███████▌  | 19/25 [00:09<00:02,  2.04it/s, eval_psnr=12.1, total_loss=0.084, train_psnr=11]    Epoch 2:  76%|███████▌  | 19/25 [00:09<00:02,  2.04it/s, eval_psnr=12.1, total_loss=0.084, train_psnr=11]    Epoch 2:  76%|███████▌  | 19/25 [00:09<00:02,  2.04it/s, eval_psnr=12.1, total_loss=0.0842, train_psnr=11]    Epoch 2:  80%|████████  | 20/25 [00:09<00:02,  2.04it/s, eval_psnr=12.1, total_loss=0.0842, train_psnr=11]    Epoch 2:  80%|████████  | 20/25 [00:09<00:02,  2.04it/s, eval_psnr=12.1, total_loss=0.0842, train_psnr=11]    Epoch 2:  80%|████████  | 20/25 [00:10<00:02,  2.04it/s, eval_psnr=12.1, total_loss=0.0838, train_psnr=11]    Epoch 2:  84%|████████▍ | 21/25 [00:10<00:01,  2.04it/s, eval_psnr=12.1, total_loss=0.0838, train_psnr=11]    Epoch 2:  84%|████████▍ | 21/25 [00:10<00:01,  2.04it/s, eval_psnr=12.1, total_loss=0.0838, train_psnr=11]    Epoch 2:  84%|████████▍ | 21/25 [00:10<00:01,  2.04it/s, eval_psnr=12.1, total_loss=0.0837, train_psnr=11]    Epoch 2:  88%|████████▊ | 22/25 [00:10<00:01,  2.02it/s, eval_psnr=12.1, total_loss=0.0837, train_psnr=11]    Epoch 2:  88%|████████▊ | 22/25 [00:10<00:01,  2.02it/s, eval_psnr=12.1, total_loss=0.0837, train_psnr=11]    Epoch 2:  88%|████████▊ | 22/25 [00:11<00:01,  2.02it/s, eval_psnr=12.1, total_loss=0.0836, train_psnr=11]    Epoch 2:  92%|█████████▏| 23/25 [00:11<00:01,  1.99it/s, eval_psnr=12.1, total_loss=0.0836, train_psnr=11]    Epoch 2:  92%|█████████▏| 23/25 [00:11<00:01,  1.99it/s, eval_psnr=12.1, total_loss=0.0836, train_psnr=11]    Epoch 2:  92%|█████████▏| 23/25 [00:11<00:01,  1.99it/s, eval_psnr=12.1, total_loss=0.083, train_psnr=11.1]    Epoch 2:  96%|█████████▌| 24/25 [00:11<00:00,  2.00it/s, eval_psnr=12.1, total_loss=0.083, train_psnr=11.1]    Epoch 2:  96%|█████████▌| 24/25 [00:11<00:00,  2.00it/s, eval_psnr=12.1, total_loss=0.083, train_psnr=11.1]    Epoch 2:  96%|█████████▌| 24/25 [00:12<00:00,  2.00it/s, eval_psnr=12.1, total_loss=0.083, train_psnr=11.1]    Epoch 2: 100%|██████████| 25/25 [00:12<00:00,  2.00it/s, eval_psnr=12.1, total_loss=0.083, train_psnr=11.1]    Epoch 2: 100%|██████████| 25/25 [00:12<00:00,  2.04it/s, eval_psnr=12.1, total_loss=0.083, train_psnr=11.1]
      0%|          | 0/25 [00:00<?, ?it/s]    Epoch 3:   0%|          | 0/25 [00:00<?, ?it/s]    Epoch 3:   0%|          | 0/25 [00:00<?, ?it/s, eval_psnr=12.1, total_loss=0.0896, train_psnr=10.7]    Epoch 3:   4%|▍         | 1/25 [00:00<00:12,  1.99it/s, eval_psnr=12.1, total_loss=0.0896, train_psnr=10.7]    Epoch 3:   4%|▍         | 1/25 [00:00<00:12,  1.99it/s, eval_psnr=12.1, total_loss=0.0896, train_psnr=10.7]    Epoch 3:   4%|▍         | 1/25 [00:01<00:12,  1.99it/s, eval_psnr=12.1, total_loss=0.0815, train_psnr=11.2]    Epoch 3:   8%|▊         | 2/25 [00:01<00:11,  1.98it/s, eval_psnr=12.1, total_loss=0.0815, train_psnr=11.2]    Epoch 3:   8%|▊         | 2/25 [00:01<00:11,  1.98it/s, eval_psnr=12.1, total_loss=0.0815, train_psnr=11.2]    Epoch 3:   8%|▊         | 2/25 [00:01<00:11,  1.98it/s, eval_psnr=12.1, total_loss=0.0837, train_psnr=11.1]    Epoch 3:  12%|█▏        | 3/25 [00:01<00:11,  1.99it/s, eval_psnr=12.1, total_loss=0.0837, train_psnr=11.1]    Epoch 3:  12%|█▏        | 3/25 [00:01<00:11,  1.99it/s, eval_psnr=12.1, total_loss=0.0837, train_psnr=11.1]    Epoch 3:  12%|█▏        | 3/25 [00:02<00:11,  1.99it/s, eval_psnr=12.1, total_loss=0.0847, train_psnr=11]      Epoch 3:  16%|█▌        | 4/25 [00:02<00:10,  1.96it/s, eval_psnr=12.1, total_loss=0.0847, train_psnr=11]    Epoch 3:  16%|█▌        | 4/25 [00:02<00:10,  1.96it/s, eval_psnr=12.1, total_loss=0.0847, train_psnr=11]    Epoch 3:  16%|█▌        | 4/25 [00:02<00:10,  1.96it/s, eval_psnr=12.1, total_loss=0.0845, train_psnr=11]    Epoch 3:  20%|██        | 5/25 [00:02<00:10,  1.97it/s, eval_psnr=12.1, total_loss=0.0845, train_psnr=11]    Epoch 3:  20%|██        | 5/25 [00:02<00:10,  1.97it/s, eval_psnr=12.1, total_loss=0.0845, train_psnr=11]    Epoch 3:  20%|██        | 5/25 [00:03<00:10,  1.97it/s, eval_psnr=12.1, total_loss=0.0844, train_psnr=11]    Epoch 3:  24%|██▍       | 6/25 [00:03<00:09,  1.97it/s, eval_psnr=12.1, total_loss=0.0844, train_psnr=11]    Epoch 3:  24%|██▍       | 6/25 [00:03<00:09,  1.97it/s, eval_psnr=12.1, total_loss=0.0844, train_psnr=11]    Epoch 3:  24%|██▍       | 6/25 [00:03<00:09,  1.97it/s, eval_psnr=12.1, total_loss=0.0848, train_psnr=11]    Epoch 3:  28%|██▊       | 7/25 [00:03<00:09,  1.99it/s, eval_psnr=12.1, total_loss=0.0848, train_psnr=11]    Epoch 3:  28%|██▊       | 7/25 [00:03<00:09,  1.99it/s, eval_psnr=12.1, total_loss=0.0848, train_psnr=11]    Epoch 3:  28%|██▊       | 7/25 [00:04<00:09,  1.99it/s, eval_psnr=12.1, total_loss=0.0834, train_psnr=11.1]    Epoch 3:  32%|███▏      | 8/25 [00:04<00:08,  2.00it/s, eval_psnr=12.1, total_loss=0.0834, train_psnr=11.1]    Epoch 3:  32%|███▏      | 8/25 [00:04<00:08,  2.00it/s, eval_psnr=12.1, total_loss=0.0834, train_psnr=11.1]    Epoch 3:  32%|███▏      | 8/25 [00:04<00:08,  2.00it/s, eval_psnr=12.1, total_loss=0.0829, train_psnr=11.1]    Epoch 3:  36%|███▌      | 9/25 [00:04<00:07,  2.01it/s, eval_psnr=12.1, total_loss=0.0829, train_psnr=11.1]    Epoch 3:  36%|███▌      | 9/25 [00:04<00:07,  2.01it/s, eval_psnr=12.1, total_loss=0.0829, train_psnr=11.1]    Epoch 3:  36%|███▌      | 9/25 [00:05<00:07,  2.01it/s, eval_psnr=12.1, total_loss=0.083, train_psnr=11.1]     Epoch 3:  40%|████      | 10/25 [00:05<00:07,  2.01it/s, eval_psnr=12.1, total_loss=0.083, train_psnr=11.1]    Epoch 3:  40%|████      | 10/25 [00:05<00:07,  2.01it/s, eval_psnr=12.1, total_loss=0.083, train_psnr=11.1]    Epoch 3:  40%|████      | 10/25 [00:05<00:07,  2.01it/s, eval_psnr=12.1, total_loss=0.0832, train_psnr=11.1]    Epoch 3:  44%|████▍     | 11/25 [00:05<00:06,  2.02it/s, eval_psnr=12.1, total_loss=0.0832, train_psnr=11.1]    Epoch 3:  44%|████▍     | 11/25 [00:05<00:06,  2.02it/s, eval_psnr=12.1, total_loss=0.0832, train_psnr=11.1]    Epoch 3:  44%|████▍     | 11/25 [00:05<00:06,  2.02it/s, eval_psnr=12.1, total_loss=0.083, train_psnr=11.1]     Epoch 3:  48%|████▊     | 12/25 [00:05<00:06,  2.03it/s, eval_psnr=12.1, total_loss=0.083, train_psnr=11.1]    Epoch 3:  48%|████▊     | 12/25 [00:05<00:06,  2.03it/s, eval_psnr=12.1, total_loss=0.083, train_psnr=11.1]    Epoch 3:  48%|████▊     | 12/25 [00:06<00:06,  2.03it/s, eval_psnr=12.1, total_loss=0.0833, train_psnr=11.1]    Epoch 3:  52%|█████▏    | 13/25 [00:06<00:05,  2.01it/s, eval_psnr=12.1, total_loss=0.0833, train_psnr=11.1]    Epoch 3:  52%|█████▏    | 13/25 [00:06<00:05,  2.01it/s, eval_psnr=12.1, total_loss=0.0833, train_psnr=11.1]    Epoch 3:  52%|█████▏    | 13/25 [00:06<00:05,  2.01it/s, eval_psnr=12.1, total_loss=0.0838, train_psnr=11]      Epoch 3:  56%|█████▌    | 14/25 [00:06<00:05,  2.02it/s, eval_psnr=12.1, total_loss=0.0838, train_psnr=11]    Epoch 3:  56%|█████▌    | 14/25 [00:06<00:05,  2.02it/s, eval_psnr=12.1, total_loss=0.0838, train_psnr=11]    Epoch 3:  56%|█████▌    | 14/25 [00:07<00:05,  2.02it/s, eval_psnr=12.1, total_loss=0.0829, train_psnr=11.1]    Epoch 3:  60%|██████    | 15/25 [00:07<00:04,  2.03it/s, eval_psnr=12.1, total_loss=0.0829, train_psnr=11.1]    Epoch 3:  60%|██████    | 15/25 [00:07<00:04,  2.03it/s, eval_psnr=12.1, total_loss=0.0829, train_psnr=11.1]    Epoch 3:  60%|██████    | 15/25 [00:07<00:04,  2.03it/s, eval_psnr=12.1, total_loss=0.083, train_psnr=11.1]     Epoch 3:  64%|██████▍   | 16/25 [00:07<00:04,  2.04it/s, eval_psnr=12.1, total_loss=0.083, train_psnr=11.1]    Epoch 3:  64%|██████▍   | 16/25 [00:07<00:04,  2.04it/s, eval_psnr=12.1, total_loss=0.083, train_psnr=11.1]    Epoch 3:  64%|██████▍   | 16/25 [00:08<00:04,  2.04it/s, eval_psnr=12.1, total_loss=0.0831, train_psnr=11.1]    Epoch 3:  68%|██████▊   | 17/25 [00:08<00:03,  2.02it/s, eval_psnr=12.1, total_loss=0.0831, train_psnr=11.1]    Epoch 3:  68%|██████▊   | 17/25 [00:08<00:03,  2.02it/s, eval_psnr=12.1, total_loss=0.0831, train_psnr=11.1]    Epoch 3:  68%|██████▊   | 17/25 [00:08<00:03,  2.02it/s, eval_psnr=12.1, total_loss=0.0836, train_psnr=11.1]    Epoch 3:  72%|███████▏  | 18/25 [00:08<00:03,  2.02it/s, eval_psnr=12.1, total_loss=0.0836, train_psnr=11.1]    Epoch 3:  72%|███████▏  | 18/25 [00:08<00:03,  2.02it/s, eval_psnr=12.1, total_loss=0.0836, train_psnr=11.1]    Epoch 3:  72%|███████▏  | 18/25 [00:09<00:03,  2.02it/s, eval_psnr=12.1, total_loss=0.0829, train_psnr=11.1]    Epoch 3:  76%|███████▌  | 19/25 [00:09<00:02,  2.02it/s, eval_psnr=12.1, total_loss=0.0829, train_psnr=11.1]    Epoch 3:  76%|███████▌  | 19/25 [00:09<00:02,  2.02it/s, eval_psnr=12.1, total_loss=0.0829, train_psnr=11.1]    Epoch 3:  76%|███████▌  | 19/25 [00:09<00:02,  2.02it/s, eval_psnr=12.1, total_loss=0.0826, train_psnr=11.1]    Epoch 3:  80%|████████  | 20/25 [00:09<00:02,  2.03it/s, eval_psnr=12.1, total_loss=0.0826, train_psnr=11.1]    Epoch 3:  80%|████████  | 20/25 [00:09<00:02,  2.03it/s, eval_psnr=12.1, total_loss=0.0826, train_psnr=11.1]    Epoch 3:  80%|████████  | 20/25 [00:10<00:02,  2.03it/s, eval_psnr=12.1, total_loss=0.082, train_psnr=11.1]     Epoch 3:  84%|████████▍ | 21/25 [00:10<00:01,  2.03it/s, eval_psnr=12.1, total_loss=0.082, train_psnr=11.1]    Epoch 3:  84%|████████▍ | 21/25 [00:10<00:01,  2.03it/s, eval_psnr=12.1, total_loss=0.082, train_psnr=11.1]    Epoch 3:  84%|████████▍ | 21/25 [00:10<00:01,  2.03it/s, eval_psnr=12.1, total_loss=0.0819, train_psnr=11.1]    Epoch 3:  88%|████████▊ | 22/25 [00:10<00:01,  2.03it/s, eval_psnr=12.1, total_loss=0.0819, train_psnr=11.1]    Epoch 3:  88%|████████▊ | 22/25 [00:10<00:01,  2.03it/s, eval_psnr=12.1, total_loss=0.0819, train_psnr=11.1]    Epoch 3:  88%|████████▊ | 22/25 [00:11<00:01,  2.03it/s, eval_psnr=12.1, total_loss=0.0816, train_psnr=11.1]    Epoch 3:  92%|█████████▏| 23/25 [00:11<00:00,  2.00it/s, eval_psnr=12.1, total_loss=0.0816, train_psnr=11.1]    Epoch 3:  92%|█████████▏| 23/25 [00:11<00:00,  2.00it/s, eval_psnr=12.1, total_loss=0.0816, train_psnr=11.1]    Epoch 3:  92%|█████████▏| 23/25 [00:11<00:00,  2.00it/s, eval_psnr=12.1, total_loss=0.0817, train_psnr=11.1]    Epoch 3:  96%|█████████▌| 24/25 [00:11<00:00,  2.02it/s, eval_psnr=12.1, total_loss=0.0817, train_psnr=11.1]    Epoch 3:  96%|█████████▌| 24/25 [00:11<00:00,  2.02it/s, eval_psnr=12.1, total_loss=0.0817, train_psnr=11.1]    Epoch 3:  96%|█████████▌| 24/25 [00:12<00:00,  2.02it/s, eval_psnr=12.1, total_loss=0.0819, train_psnr=11.1]    Epoch 3: 100%|██████████| 25/25 [00:12<00:00,  2.04it/s, eval_psnr=12.1, total_loss=0.0819, train_psnr=11.1]    Epoch 3: 100%|██████████| 25/25 [00:12<00:00,  2.01it/s, eval_psnr=12.1, total_loss=0.0819, train_psnr=11.1]
      0%|          | 0/25 [00:00<?, ?it/s]    Epoch 4:   0%|          | 0/25 [00:00<?, ?it/s]    Epoch 4:   0%|          | 0/25 [00:00<?, ?it/s, eval_psnr=12.2, total_loss=0.0746, train_psnr=11.8]    Epoch 4:   4%|▍         | 1/25 [00:00<00:11,  2.06it/s, eval_psnr=12.2, total_loss=0.0746, train_psnr=11.8]    Epoch 4:   4%|▍         | 1/25 [00:00<00:11,  2.06it/s, eval_psnr=12.2, total_loss=0.0746, train_psnr=11.8]    Epoch 4:   4%|▍         | 1/25 [00:00<00:11,  2.06it/s, eval_psnr=12.2, total_loss=0.0775, train_psnr=11.5]    Epoch 4:   8%|▊         | 2/25 [00:00<00:11,  2.05it/s, eval_psnr=12.2, total_loss=0.0775, train_psnr=11.5]    Epoch 4:   8%|▊         | 2/25 [00:00<00:11,  2.05it/s, eval_psnr=12.2, total_loss=0.0775, train_psnr=11.5]    Epoch 4:   8%|▊         | 2/25 [00:01<00:11,  2.05it/s, eval_psnr=12.2, total_loss=0.0799, train_psnr=11.3]    Epoch 4:  12%|█▏        | 3/25 [00:01<00:10,  2.05it/s, eval_psnr=12.2, total_loss=0.0799, train_psnr=11.3]    Epoch 4:  12%|█▏        | 3/25 [00:01<00:10,  2.05it/s, eval_psnr=12.2, total_loss=0.0799, train_psnr=11.3]    Epoch 4:  12%|█▏        | 3/25 [00:01<00:10,  2.05it/s, eval_psnr=12.2, total_loss=0.0786, train_psnr=11.3]    Epoch 4:  16%|█▌        | 4/25 [00:01<00:10,  2.05it/s, eval_psnr=12.2, total_loss=0.0786, train_psnr=11.3]    Epoch 4:  16%|█▌        | 4/25 [00:01<00:10,  2.05it/s, eval_psnr=12.2, total_loss=0.0786, train_psnr=11.3]    Epoch 4:  16%|█▌        | 4/25 [00:02<00:10,  2.05it/s, eval_psnr=12.2, total_loss=0.0805, train_psnr=11.2]    Epoch 4:  20%|██        | 5/25 [00:02<00:09,  2.05it/s, eval_psnr=12.2, total_loss=0.0805, train_psnr=11.2]    Epoch 4:  20%|██        | 5/25 [00:02<00:09,  2.05it/s, eval_psnr=12.2, total_loss=0.0805, train_psnr=11.2]    Epoch 4:  20%|██        | 5/25 [00:02<00:09,  2.05it/s, eval_psnr=12.2, total_loss=0.0791, train_psnr=11.3]    Epoch 4:  24%|██▍       | 6/25 [00:02<00:09,  2.05it/s, eval_psnr=12.2, total_loss=0.0791, train_psnr=11.3]    Epoch 4:  24%|██▍       | 6/25 [00:02<00:09,  2.05it/s, eval_psnr=12.2, total_loss=0.0791, train_psnr=11.3]    Epoch 4:  24%|██▍       | 6/25 [00:03<00:09,  2.05it/s, eval_psnr=12.2, total_loss=0.0822, train_psnr=11.1]    Epoch 4:  28%|██▊       | 7/25 [00:03<00:08,  2.03it/s, eval_psnr=12.2, total_loss=0.0822, train_psnr=11.1]    Epoch 4:  28%|██▊       | 7/25 [00:03<00:08,  2.03it/s, eval_psnr=12.2, total_loss=0.0822, train_psnr=11.1]    Epoch 4:  28%|██▊       | 7/25 [00:03<00:08,  2.03it/s, eval_psnr=12.2, total_loss=0.0832, train_psnr=11.1]    Epoch 4:  32%|███▏      | 8/25 [00:03<00:08,  2.04it/s, eval_psnr=12.2, total_loss=0.0832, train_psnr=11.1]    Epoch 4:  32%|███▏      | 8/25 [00:03<00:08,  2.04it/s, eval_psnr=12.2, total_loss=0.0832, train_psnr=11.1]    Epoch 4:  32%|███▏      | 8/25 [00:04<00:08,  2.04it/s, eval_psnr=12.2, total_loss=0.0827, train_psnr=11.1]    Epoch 4:  36%|███▌      | 9/25 [00:04<00:07,  2.04it/s, eval_psnr=12.2, total_loss=0.0827, train_psnr=11.1]    Epoch 4:  36%|███▌      | 9/25 [00:04<00:07,  2.04it/s, eval_psnr=12.2, total_loss=0.0827, train_psnr=11.1]    Epoch 4:  36%|███▌      | 9/25 [00:04<00:07,  2.04it/s, eval_psnr=12.2, total_loss=0.081, train_psnr=11.2]     Epoch 4:  40%|████      | 10/25 [00:04<00:07,  2.05it/s, eval_psnr=12.2, total_loss=0.081, train_psnr=11.2]    Epoch 4:  40%|████      | 10/25 [00:04<00:07,  2.05it/s, eval_psnr=12.2, total_loss=0.081, train_psnr=11.2]    Epoch 4:  40%|████      | 10/25 [00:05<00:07,  2.05it/s, eval_psnr=12.2, total_loss=0.0809, train_psnr=11.2]    Epoch 4:  44%|████▍     | 11/25 [00:05<00:06,  2.02it/s, eval_psnr=12.2, total_loss=0.0809, train_psnr=11.2]    Epoch 4:  44%|████▍     | 11/25 [00:05<00:06,  2.02it/s, eval_psnr=12.2, total_loss=0.0809, train_psnr=11.2]    Epoch 4:  44%|████▍     | 11/25 [00:05<00:06,  2.02it/s, eval_psnr=12.2, total_loss=0.0821, train_psnr=11.1]    Epoch 4:  48%|████▊     | 12/25 [00:05<00:06,  2.03it/s, eval_psnr=12.2, total_loss=0.0821, train_psnr=11.1]    Epoch 4:  48%|████▊     | 12/25 [00:05<00:06,  2.03it/s, eval_psnr=12.2, total_loss=0.0821, train_psnr=11.1]    Epoch 4:  48%|████▊     | 12/25 [00:06<00:06,  2.03it/s, eval_psnr=12.2, total_loss=0.083, train_psnr=11.1]     Epoch 4:  52%|█████▏    | 13/25 [00:06<00:05,  2.03it/s, eval_psnr=12.2, total_loss=0.083, train_psnr=11.1]    Epoch 4:  52%|█████▏    | 13/25 [00:06<00:05,  2.03it/s, eval_psnr=12.2, total_loss=0.083, train_psnr=11.1]    Epoch 4:  52%|█████▏    | 13/25 [00:06<00:05,  2.03it/s, eval_psnr=12.2, total_loss=0.0823, train_psnr=11.1]    Epoch 4:  56%|█████▌    | 14/25 [00:06<00:05,  2.04it/s, eval_psnr=12.2, total_loss=0.0823, train_psnr=11.1]    Epoch 4:  56%|█████▌    | 14/25 [00:06<00:05,  2.04it/s, eval_psnr=12.2, total_loss=0.0823, train_psnr=11.1]    Epoch 4:  56%|█████▌    | 14/25 [00:07<00:05,  2.04it/s, eval_psnr=12.2, total_loss=0.0812, train_psnr=11.2]    Epoch 4:  60%|██████    | 15/25 [00:07<00:04,  2.01it/s, eval_psnr=12.2, total_loss=0.0812, train_psnr=11.2]    Epoch 4:  60%|██████    | 15/25 [00:07<00:04,  2.01it/s, eval_psnr=12.2, total_loss=0.0812, train_psnr=11.2]    Epoch 4:  60%|██████    | 15/25 [00:07<00:04,  2.01it/s, eval_psnr=12.2, total_loss=0.0825, train_psnr=11.1]    Epoch 4:  64%|██████▍   | 16/25 [00:07<00:04,  2.02it/s, eval_psnr=12.2, total_loss=0.0825, train_psnr=11.1]    Epoch 4:  64%|██████▍   | 16/25 [00:07<00:04,  2.02it/s, eval_psnr=12.2, total_loss=0.0825, train_psnr=11.1]    Epoch 4:  64%|██████▍   | 16/25 [00:08<00:04,  2.02it/s, eval_psnr=12.2, total_loss=0.0826, train_psnr=11.1]    Epoch 4:  68%|██████▊   | 17/25 [00:08<00:03,  2.03it/s, eval_psnr=12.2, total_loss=0.0826, train_psnr=11.1]    Epoch 4:  68%|██████▊   | 17/25 [00:08<00:03,  2.03it/s, eval_psnr=12.2, total_loss=0.0826, train_psnr=11.1]    Epoch 4:  68%|██████▊   | 17/25 [00:08<00:03,  2.03it/s, eval_psnr=12.2, total_loss=0.0819, train_psnr=11.1]    Epoch 4:  72%|███████▏  | 18/25 [00:08<00:03,  2.03it/s, eval_psnr=12.2, total_loss=0.0819, train_psnr=11.1]    Epoch 4:  72%|███████▏  | 18/25 [00:08<00:03,  2.03it/s, eval_psnr=12.2, total_loss=0.0819, train_psnr=11.1]    Epoch 4:  72%|███████▏  | 18/25 [00:09<00:03,  2.03it/s, eval_psnr=12.2, total_loss=0.0819, train_psnr=11.1]    Epoch 4:  76%|███████▌  | 19/25 [00:09<00:02,  2.01it/s, eval_psnr=12.2, total_loss=0.0819, train_psnr=11.1]    Epoch 4:  76%|███████▌  | 19/25 [00:09<00:02,  2.01it/s, eval_psnr=12.2, total_loss=0.0819, train_psnr=11.1]    Epoch 4:  76%|███████▌  | 19/25 [00:09<00:02,  2.01it/s, eval_psnr=12.2, total_loss=0.0823, train_psnr=11.1]    Epoch 4:  80%|████████  | 20/25 [00:09<00:02,  2.02it/s, eval_psnr=12.2, total_loss=0.0823, train_psnr=11.1]    Epoch 4:  80%|████████  | 20/25 [00:09<00:02,  2.02it/s, eval_psnr=12.2, total_loss=0.0823, train_psnr=11.1]    Epoch 4:  80%|████████  | 20/25 [00:10<00:02,  2.02it/s, eval_psnr=12.2, total_loss=0.0815, train_psnr=11.1]    Epoch 4:  84%|████████▍ | 21/25 [00:10<00:01,  2.04it/s, eval_psnr=12.2, total_loss=0.0815, train_psnr=11.1]    Epoch 4:  84%|████████▍ | 21/25 [00:10<00:01,  2.04it/s, eval_psnr=12.2, total_loss=0.0815, train_psnr=11.1]    Epoch 4:  84%|████████▍ | 21/25 [00:10<00:01,  2.04it/s, eval_psnr=12.2, total_loss=0.0819, train_psnr=11.1]    Epoch 4:  88%|████████▊ | 22/25 [00:10<00:01,  2.05it/s, eval_psnr=12.2, total_loss=0.0819, train_psnr=11.1]    Epoch 4:  88%|████████▊ | 22/25 [00:10<00:01,  2.05it/s, eval_psnr=12.2, total_loss=0.0819, train_psnr=11.1]    Epoch 4:  88%|████████▊ | 22/25 [00:11<00:01,  2.05it/s, eval_psnr=12.2, total_loss=0.0812, train_psnr=11.2]    Epoch 4:  92%|█████████▏| 23/25 [00:11<00:00,  2.05it/s, eval_psnr=12.2, total_loss=0.0812, train_psnr=11.2]    Epoch 4:  92%|█████████▏| 23/25 [00:11<00:00,  2.05it/s, eval_psnr=12.2, total_loss=0.0812, train_psnr=11.2]    Epoch 4:  92%|█████████▏| 23/25 [00:11<00:00,  2.05it/s, eval_psnr=12.2, total_loss=0.0817, train_psnr=11.1]    Epoch 4:  96%|█████████▌| 24/25 [00:11<00:00,  2.04it/s, eval_psnr=12.2, total_loss=0.0817, train_psnr=11.1]    Epoch 4:  96%|█████████▌| 24/25 [00:11<00:00,  2.04it/s, eval_psnr=12.2, total_loss=0.0817, train_psnr=11.1]    Epoch 4:  96%|█████████▌| 24/25 [00:12<00:00,  2.04it/s, eval_psnr=12.2, total_loss=0.0817, train_psnr=11.1]    Epoch 4: 100%|██████████| 25/25 [00:12<00:00,  2.04it/s, eval_psnr=12.2, total_loss=0.0817, train_psnr=11.1]    Epoch 4: 100%|██████████| 25/25 [00:12<00:00,  2.04it/s, eval_psnr=12.2, total_loss=0.0817, train_psnr=11.1]
      0%|          | 0/25 [00:00<?, ?it/s]    Epoch 5:   0%|          | 0/25 [00:00<?, ?it/s]    Epoch 5:   0%|          | 0/25 [00:00<?, ?it/s, eval_psnr=12.2, total_loss=0.0774, train_psnr=11.4]    Epoch 5:   4%|▍         | 1/25 [00:00<00:11,  2.06it/s, eval_psnr=12.2, total_loss=0.0774, train_psnr=11.4]    Epoch 5:   4%|▍         | 1/25 [00:00<00:11,  2.06it/s, eval_psnr=12.2, total_loss=0.0774, train_psnr=11.4]    Epoch 5:   4%|▍         | 1/25 [00:00<00:11,  2.06it/s, eval_psnr=12.2, total_loss=0.072, train_psnr=11.7]     Epoch 5:   8%|▊         | 2/25 [00:00<00:11,  2.08it/s, eval_psnr=12.2, total_loss=0.072, train_psnr=11.7]    Epoch 5:   8%|▊         | 2/25 [00:00<00:11,  2.08it/s, eval_psnr=12.2, total_loss=0.072, train_psnr=11.7]    Epoch 5:   8%|▊         | 2/25 [00:01<00:11,  2.08it/s, eval_psnr=12.2, total_loss=0.0778, train_psnr=11.3]    Epoch 5:  12%|█▏        | 3/25 [00:01<00:10,  2.03it/s, eval_psnr=12.2, total_loss=0.0778, train_psnr=11.3]    Epoch 5:  12%|█▏        | 3/25 [00:01<00:10,  2.03it/s, eval_psnr=12.2, total_loss=0.0778, train_psnr=11.3]    Epoch 5:  12%|█▏        | 3/25 [00:01<00:10,  2.03it/s, eval_psnr=12.2, total_loss=0.0776, train_psnr=11.3]    Epoch 5:  16%|█▌        | 4/25 [00:01<00:10,  2.05it/s, eval_psnr=12.2, total_loss=0.0776, train_psnr=11.3]    Epoch 5:  16%|█▌        | 4/25 [00:01<00:10,  2.05it/s, eval_psnr=12.2, total_loss=0.0776, train_psnr=11.3]    Epoch 5:  16%|█▌        | 4/25 [00:02<00:10,  2.05it/s, eval_psnr=12.2, total_loss=0.0772, train_psnr=11.3]    Epoch 5:  20%|██        | 5/25 [00:02<00:09,  2.06it/s, eval_psnr=12.2, total_loss=0.0772, train_psnr=11.3]    Epoch 5:  20%|██        | 5/25 [00:02<00:09,  2.06it/s, eval_psnr=12.2, total_loss=0.0772, train_psnr=11.3]    Epoch 5:  20%|██        | 5/25 [00:02<00:09,  2.06it/s, eval_psnr=12.2, total_loss=0.0763, train_psnr=11.4]    Epoch 5:  24%|██▍       | 6/25 [00:02<00:09,  2.06it/s, eval_psnr=12.2, total_loss=0.0763, train_psnr=11.4]    Epoch 5:  24%|██▍       | 6/25 [00:02<00:09,  2.06it/s, eval_psnr=12.2, total_loss=0.0763, train_psnr=11.4]    Epoch 5:  24%|██▍       | 6/25 [00:03<00:09,  2.06it/s, eval_psnr=12.2, total_loss=0.0752, train_psnr=11.4]    Epoch 5:  28%|██▊       | 7/25 [00:03<00:08,  2.03it/s, eval_psnr=12.2, total_loss=0.0752, train_psnr=11.4]    Epoch 5:  28%|██▊       | 7/25 [00:03<00:08,  2.03it/s, eval_psnr=12.2, total_loss=0.0752, train_psnr=11.4]    Epoch 5:  28%|██▊       | 7/25 [00:03<00:08,  2.03it/s, eval_psnr=12.2, total_loss=0.0763, train_psnr=11.4]    Epoch 5:  32%|███▏      | 8/25 [00:03<00:08,  2.05it/s, eval_psnr=12.2, total_loss=0.0763, train_psnr=11.4]    Epoch 5:  32%|███▏      | 8/25 [00:03<00:08,  2.05it/s, eval_psnr=12.2, total_loss=0.0763, train_psnr=11.4]    Epoch 5:  32%|███▏      | 8/25 [00:04<00:08,  2.05it/s, eval_psnr=12.2, total_loss=0.0782, train_psnr=11.3]    Epoch 5:  36%|███▌      | 9/25 [00:04<00:07,  2.05it/s, eval_psnr=12.2, total_loss=0.0782, train_psnr=11.3]    Epoch 5:  36%|███▌      | 9/25 [00:04<00:07,  2.05it/s, eval_psnr=12.2, total_loss=0.0782, train_psnr=11.3]    Epoch 5:  36%|███▌      | 9/25 [00:04<00:07,  2.05it/s, eval_psnr=12.2, total_loss=0.0791, train_psnr=11.2]    Epoch 5:  40%|████      | 10/25 [00:04<00:07,  2.06it/s, eval_psnr=12.2, total_loss=0.0791, train_psnr=11.2]    Epoch 5:  40%|████      | 10/25 [00:04<00:07,  2.06it/s, eval_psnr=12.2, total_loss=0.0791, train_psnr=11.2]    Epoch 5:  40%|████      | 10/25 [00:05<00:07,  2.06it/s, eval_psnr=12.2, total_loss=0.0798, train_psnr=11.2]    Epoch 5:  44%|████▍     | 11/25 [00:05<00:06,  2.04it/s, eval_psnr=12.2, total_loss=0.0798, train_psnr=11.2]    Epoch 5:  44%|████▍     | 11/25 [00:05<00:06,  2.04it/s, eval_psnr=12.2, total_loss=0.0798, train_psnr=11.2]    Epoch 5:  44%|████▍     | 11/25 [00:05<00:06,  2.04it/s, eval_psnr=12.2, total_loss=0.0797, train_psnr=11.2]    Epoch 5:  48%|████▊     | 12/25 [00:05<00:06,  2.04it/s, eval_psnr=12.2, total_loss=0.0797, train_psnr=11.2]    Epoch 5:  48%|████▊     | 12/25 [00:05<00:06,  2.04it/s, eval_psnr=12.2, total_loss=0.0797, train_psnr=11.2]    Epoch 5:  48%|████▊     | 12/25 [00:06<00:06,  2.04it/s, eval_psnr=12.2, total_loss=0.0807, train_psnr=11.2]    Epoch 5:  52%|█████▏    | 13/25 [00:06<00:05,  2.04it/s, eval_psnr=12.2, total_loss=0.0807, train_psnr=11.2]    Epoch 5:  52%|█████▏    | 13/25 [00:06<00:05,  2.04it/s, eval_psnr=12.2, total_loss=0.0807, train_psnr=11.2]    Epoch 5:  52%|█████▏    | 13/25 [00:06<00:05,  2.04it/s, eval_psnr=12.2, total_loss=0.0799, train_psnr=11.2]    Epoch 5:  56%|█████▌    | 14/25 [00:06<00:05,  2.02it/s, eval_psnr=12.2, total_loss=0.0799, train_psnr=11.2]    Epoch 5:  56%|█████▌    | 14/25 [00:06<00:05,  2.02it/s, eval_psnr=12.2, total_loss=0.0799, train_psnr=11.2]    Epoch 5:  56%|█████▌    | 14/25 [00:07<00:05,  2.02it/s, eval_psnr=12.2, total_loss=0.0802, train_psnr=11.2]    Epoch 5:  60%|██████    | 15/25 [00:07<00:04,  2.03it/s, eval_psnr=12.2, total_loss=0.0802, train_psnr=11.2]    Epoch 5:  60%|██████    | 15/25 [00:07<00:04,  2.03it/s, eval_psnr=12.2, total_loss=0.0802, train_psnr=11.2]    Epoch 5:  60%|██████    | 15/25 [00:07<00:04,  2.03it/s, eval_psnr=12.2, total_loss=0.0797, train_psnr=11.2]    Epoch 5:  64%|██████▍   | 16/25 [00:07<00:04,  2.03it/s, eval_psnr=12.2, total_loss=0.0797, train_psnr=11.2]    Epoch 5:  64%|██████▍   | 16/25 [00:07<00:04,  2.03it/s, eval_psnr=12.2, total_loss=0.0797, train_psnr=11.2]    Epoch 5:  64%|██████▍   | 16/25 [00:08<00:04,  2.03it/s, eval_psnr=12.2, total_loss=0.0797, train_psnr=11.2]    Epoch 5:  68%|██████▊   | 17/25 [00:08<00:03,  2.04it/s, eval_psnr=12.2, total_loss=0.0797, train_psnr=11.2]    Epoch 5:  68%|██████▊   | 17/25 [00:08<00:03,  2.04it/s, eval_psnr=12.2, total_loss=0.0797, train_psnr=11.2]    Epoch 5:  68%|██████▊   | 17/25 [00:08<00:03,  2.04it/s, eval_psnr=12.2, total_loss=0.0809, train_psnr=11.2]    Epoch 5:  72%|███████▏  | 18/25 [00:08<00:03,  2.04it/s, eval_psnr=12.2, total_loss=0.0809, train_psnr=11.2]    Epoch 5:  72%|███████▏  | 18/25 [00:08<00:03,  2.04it/s, eval_psnr=12.2, total_loss=0.0809, train_psnr=11.2]    Epoch 5:  72%|███████▏  | 18/25 [00:09<00:03,  2.04it/s, eval_psnr=12.2, total_loss=0.0809, train_psnr=11.2]    Epoch 5:  76%|███████▌  | 19/25 [00:09<00:02,  2.05it/s, eval_psnr=12.2, total_loss=0.0809, train_psnr=11.2]    Epoch 5:  76%|███████▌  | 19/25 [00:09<00:02,  2.05it/s, eval_psnr=12.2, total_loss=0.0809, train_psnr=11.2]    Epoch 5:  76%|███████▌  | 19/25 [00:09<00:02,  2.05it/s, eval_psnr=12.2, total_loss=0.0818, train_psnr=11.1]    Epoch 5:  80%|████████  | 20/25 [00:09<00:02,  2.06it/s, eval_psnr=12.2, total_loss=0.0818, train_psnr=11.1]    Epoch 5:  80%|████████  | 20/25 [00:09<00:02,  2.06it/s, eval_psnr=12.2, total_loss=0.0818, train_psnr=11.1]    Epoch 5:  80%|████████  | 20/25 [00:10<00:02,  2.06it/s, eval_psnr=12.2, total_loss=0.082, train_psnr=11.1]     Epoch 5:  84%|████████▍ | 21/25 [00:10<00:01,  2.06it/s, eval_psnr=12.2, total_loss=0.082, train_psnr=11.1]    Epoch 5:  84%|████████▍ | 21/25 [00:10<00:01,  2.06it/s, eval_psnr=12.2, total_loss=0.082, train_psnr=11.1]    Epoch 5:  84%|████████▍ | 21/25 [00:10<00:01,  2.06it/s, eval_psnr=12.2, total_loss=0.0817, train_psnr=11.1]    Epoch 5:  88%|████████▊ | 22/25 [00:10<00:01,  2.06it/s, eval_psnr=12.2, total_loss=0.0817, train_psnr=11.1]    Epoch 5:  88%|████████▊ | 22/25 [00:10<00:01,  2.06it/s, eval_psnr=12.2, total_loss=0.0817, train_psnr=11.1]    Epoch 5:  88%|████████▊ | 22/25 [00:11<00:01,  2.06it/s, eval_psnr=12.2, total_loss=0.0815, train_psnr=11.2]    Epoch 5:  92%|█████████▏| 23/25 [00:11<00:00,  2.07it/s, eval_psnr=12.2, total_loss=0.0815, train_psnr=11.2]    Epoch 5:  92%|█████████▏| 23/25 [00:11<00:00,  2.07it/s, eval_psnr=12.2, total_loss=0.0815, train_psnr=11.2]    Epoch 5:  92%|█████████▏| 23/25 [00:11<00:00,  2.07it/s, eval_psnr=12.2, total_loss=0.0812, train_psnr=11.2]    Epoch 5:  96%|█████████▌| 24/25 [00:11<00:00,  2.06it/s, eval_psnr=12.2, total_loss=0.0812, train_psnr=11.2]    Epoch 5:  96%|█████████▌| 24/25 [00:11<00:00,  2.06it/s, eval_psnr=12.2, total_loss=0.0812, train_psnr=11.2]    Epoch 5:  96%|█████████▌| 24/25 [00:12<00:00,  2.06it/s, eval_psnr=12.2, total_loss=0.0814, train_psnr=11.2]    Epoch 5: 100%|██████████| 25/25 [00:12<00:00,  2.07it/s, eval_psnr=12.2, total_loss=0.0814, train_psnr=11.2]    Epoch 5: 100%|██████████| 25/25 [00:12<00:00,  2.05it/s, eval_psnr=12.2, total_loss=0.0814, train_psnr=11.2]

    BaseUnfold(
      (fixed_point): FixedPoint(
        (iterator): PGDIteration(
          (f_step): fStepPGD()
          (g_step): gStepPGD()
        )
      )
      (init_params_algo): ParameterDict(
          (beta): Object of type: list
          (g_param): Object of type: list
          (lambda): Object of type: ParameterList
          (stepsize): Object of type: ParameterList
        (lambda): ParameterList(
            (0): Parameter containing: [torch.float32 of size ]
            (1): Parameter containing: [torch.float32 of size ]
            (2): Parameter containing: [torch.float32 of size ]
            (3): Parameter containing: [torch.float32 of size ]
            (4): Parameter containing: [torch.float32 of size ]
        )
        (stepsize): ParameterList(
            (0): Parameter containing: [torch.float32 of size ]
            (1): Parameter containing: [torch.float32 of size ]
            (2): Parameter containing: [torch.float32 of size ]
            (3): Parameter containing: [torch.float32 of size ]
            (4): Parameter containing: [torch.float32 of size ]
        )
      )
      (params_algo): ParameterDict(
          (beta): Object of type: list
          (g_param): Object of type: list
          (lambda): Object of type: ParameterList
          (stepsize): Object of type: ParameterList
        (lambda): ParameterList(
            (0): Parameter containing: [torch.float32 of size ]
            (1): Parameter containing: [torch.float32 of size ]
            (2): Parameter containing: [torch.float32 of size ]
            (3): Parameter containing: [torch.float32 of size ]
            (4): Parameter containing: [torch.float32 of size ]
        )
        (stepsize): ParameterList(
            (0): Parameter containing: [torch.float32 of size ]
            (1): Parameter containing: [torch.float32 of size ]
            (2): Parameter containing: [torch.float32 of size ]
            (3): Parameter containing: [torch.float32 of size ]
            (4): Parameter containing: [torch.float32 of size ]
        )
      )
      (prior): ModuleList(
        (0): Prior()
      )
      (data_fidelity): ModuleList(
        (0): L2()
      )
    )



.. GENERATED FROM PYTHON SOURCE LINES 231-238

Test the network.
-----------------

We now test the learned unrolled network on the test dataset. In the plotted results, the `Linear` column shows the
measurements back-projected in the image domain, the `Recons` column shows the output of our LISTA network,
and `GT` shows the ground truth.


.. GENERATED FROM PYTHON SOURCE LINES 238-254

.. code-block:: Python


    plot_images = True
    method = "unfolded_pgd"

    test(
        model=model,
        test_dataloader=test_dataloader,
        physics=physics,
        device=device,
        plot_images=plot_images,
        save_folder=RESULTS_DIR / method / operation,
        verbose=verbose,
        wandb_vis=wandb_vis,
    )





.. image-sg:: /auto_examples/unfolded/images/sphx_glr_demo_custom_prior_unfolded_001.png
   :alt: No learning, Recons., GT
   :srcset: /auto_examples/unfolded/images/sphx_glr_demo_custom_prior_unfolded_001.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Processing data of operator 1 out of 1
      0%|          | 0/1 [00:00<?, ?it/s]    100%|██████████| 1/1 [00:03<00:00,  3.14s/it]    100%|██████████| 1/1 [00:03<00:00,  3.14s/it]
    Test PSNR: No learning rec.: 11.28+-0.00 dB | Model: 12.16+-0.00 dB. 

    (12.159058570861816, 0.0, 11.281496047973633, 0.0)



.. GENERATED FROM PYTHON SOURCE LINES 255-261

Plotting the weights of the network.
------------------------------------

We now plot the weights of the network that were learned and check that they are different from their initialization
values. Note that ``g_param`` corresponds to :math:`1/\lambda` in the proximal gradient algorithm.


.. GENERATED FROM PYTHON SOURCE LINES 261-265

.. code-block:: Python


    dinv.utils.plotting.plot_parameters(
        model, init_params=params_algo, save_dir=RESULTS_DIR / method / operation
    )



.. image-sg:: /auto_examples/unfolded/images/sphx_glr_demo_custom_prior_unfolded_002.png
   :alt: demo custom prior unfolded
   :srcset: /auto_examples/unfolded/images/sphx_glr_demo_custom_prior_unfolded_002.png
   :class: sphx-glr-single-img






.. rst-class:: sphx-glr-timing

   **Total running time of the script:** (1 minutes 3.195 seconds)


.. _sphx_glr_download_auto_examples_unfolded_demo_custom_prior_unfolded.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: demo_custom_prior_unfolded.ipynb <demo_custom_prior_unfolded.ipynb>`

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: demo_custom_prior_unfolded.py <demo_custom_prior_unfolded.py>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
